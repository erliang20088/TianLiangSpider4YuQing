#global config
#node_is_master=true
node_is_master=false
#最大的种子的url的个数
node_seeds_max_size=100000000
#一次性注入到redis的url任务的数量，即批量添加的条数
node_seeds_inject_batch_size=1000
#低于这个值就会自动注入新的集合到redis队列中
node_redis_size_threshold=1000
#注入一次后sleep多长时间
node_inject_urls_sleep_time=5000
#client成功抓取一个URL的休息时间
node_client_grab_success_sleep_interval_time=500
#请求失败后，可以重复请求的次数
node_req_fail_max_count=2

#httpclient  parameters
HttpGet.http.socket.timeout=5000
#最大链接数
max_connections=80
#获取链接的最大等待时间
wait_connection_timeout=15000
#链接超时时间
connection_timeout=10000
#读数据超时时间
read_timeout=20000
#遇到错误时候的最大重复请求次数
http_req_error_repeat_number=1
http_req_once_wait_time=300

#爬虫的参数设置
#爬虫种子目录，暂以文件形式来管理种子任务
spider_seeds_root_path=seeds
#为快捷简单,暂不启用该参数
threads=2
#一些数据暂存到本地文件系统中
data_dir=data
#depth=4
depth=3
max_depth=6
topN=100
max_topN=200

#add mode number
task_count_circle_keyword=4
task_count_circle_normal=9

proxy_open=false
#proxy_open=true
#代表是否启用自身IP去请求下载网页
#proxy_self=true
proxy_self=false
ip_proxy_file_path=proxy
proxy_fail_max_count=1

#crawl repeat config
crawl_page_repeat_number=1

#log config
#log_output_file_enable=true
log_output_file_enable=false

#web service binding defination
ws_server_ip=127.0.0.1
ws_server_port=9999

#关于redis的server host ip 和 port的配置
redis_host=localhost
#redis_host=101.200.216.12
redis_port=6379
redis_password=sinx/cosx=tanx

#为simple crawler添加的新配置
#表明规则文件所在的位置，是在jar包，还是在jar之外的文件系统中.in为在jar包中，out则是在文件文件系统中
#ext_content_rule_config_fs=in
ext_content_rule_config_fs=out
#此处可以写文件，也可以选择是文件夹。如果上边是in,则此处只支持文件，外部文件系统的话就无所谓了
#规则模板所存放的总目录，里边可以随意存文件或文件夹，只要符合规则即可
ext_content_rule_config_root_dir=rules
#ext_content_rule_config_root_dir=D:\\test
ext_content_save_threads_numbers=1
ext_content_save_thread_sleep_time=10000
#0代表没有更新，1代表有更新
ext_content_rule_key_is_changed=1
#时间为s
ext_content_rule_key_syn_circle=10
#是否开启循环加载种子文件
ext_content_load_seeds_is_circle=true
#循环加载种子目录时，多长时间加载一次!
ext_content_load_seeds_sleep=10

#cache data config
#unit is second
cache_data_circle_save_interval=60

#parser rule test
#application_is_test=true
application_is_test=false
#task parameters
task_circle_enable=true

#####phnatomjs start##################################
phantomjs_path=phantomjs/
phantomjs_exe_name=phantomjs
phantomjs_crawl_config_jquery_path=phantomjs/jquery-2.1.0.min.js
phantomjs_crawl_config_json_root_path=phantomjs/config_phantomjs/
phantomjs_crawl_config_js_root_path=phantomjs/js/
phantomjs_crawl_config_js_para_root_path=phantomjs/para/
#crawl param config
phantomjs_crawl_config_capture_pic_root_path_seg=phantomjs/capture/
phantomjs_crawl_config_capture_pic_root_path_body=phantomjs/capture_body/
phantomjs_crawl_config_txt_root_path_seg=phantomjs/crawlData/
phantomjs_crawl_config_txt_root_path_body=phantomjs/crawlData_body/

phantomjs_crawl_config_all_data_root_path_random_url=phantomjs/random_url/

phantomjs_crawl_config_no_response_waitting_time_max=20000
phantomjs_crawl_config_no_response_waitting_fail_time_max=0

#static value is to here
#is_inject_jquery_default=true
is_inject_jquery_default=false
is_capture_pic_default_seg=true
#is_capture_pic_default_seg=true
#is_capture_pic_default_body=true
is_capture_pic_default_body=false
is_capture_pic_default_random_page=true
#is_capture_pic_default_random_page=false
is_data_write_to_file_default=true
crawl_max_page_number=1

#抓取输出定义
phantojs_seg_output_root_path=crawl_output/
#####phnatomjs end##################################

#mysql version database setting
jdbc.driver=com.mysql.jdbc.Driver
#jdbc.url=jdbc:mysql://192.168.1.201:3306/mbmTest?useUnicode=true&zeroDateTimeBehavior=convertToNull&characterEncoding=utf-8
#jdbc.url=jdbc\:mysql\://localhost\:3306/weiboSpider?useUnicode\=true&zeroDateTimeBehavior\=convertToNull&characterEncoding\=utf-8
jdbc.url=jdbc\:mysql\://127.0.0.1\:3306/conac_brand?useUnicode\=true&zeroDateTimeBehavior\=convertToNull&characterEncoding\=utf-8
jdbc.username=root
jdbc.password=zhouking

#avoid of OOM
task_todo_level_2_max_items_in_redis=100000

#######Test########
test=成功